{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ABInBev Case - Interactive Queries\n",
                "\n",
                "Use this notebook to query the data processed by the pipeline. Tables are loaded as temporary views."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Configurar SparkSession com Delta Lake\n",
                "spark = (SparkSession.builder\n",
                "    .appName(\"ABInBev_Interactive_Query\")\n",
                "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\")\n",
                "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
                "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
                "    .getOrCreate())\n",
                "\n",
                "# Caminho base dos dados (assumindo execucao na raiz do projeto ou ajustando path)\n",
                "DATA_DIR = \"../data\" if os.path.exists(\"../data\") else \"data\"\n",
                "\n",
                "print(f\"Spark Version: {spark.version}\")\n",
                "print(f\"Data Directory: {DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Tables\n",
                "Loading tables from Silver, Gold, and Consumption layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Silver\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/silver/silver_sales_enriched\").createOrReplaceTempView(\"silver_sales\")\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/silver/silver_channel_features\").createOrReplaceTempView(\"silver_channels\")\n",
                "\n",
                "# Gold\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/gold/gold_sales_enriched\").createOrReplaceTempView(\"gold_sales\")\n",
                "\n",
                "# Consumption (Dimensions & Facts)\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/consumption/dim_date\").createOrReplaceTempView(\"dim_date\")\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/consumption/dim_product\").createOrReplaceTempView(\"dim_product\")\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/consumption/dim_region\").createOrReplaceTempView(\"dim_region\")\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/consumption/dim_channel\").createOrReplaceTempView(\"dim_channel\")\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/consumption/fact_sales\").createOrReplaceTempView(\"fact_sales\")\n",
                "\n",
                "print(\"Tables loaded and registered as Temp Views!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Business Questions\n",
                "### 2.1 Top 3 Trade Groups by Region"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query_1 = \"\"\"\n",
                "SELECT \n",
                "    r.region_name,\n",
                "    c.trade_group_desc,\n",
                "    SUM(f.dollar_volume) as total_dollar_volume\n",
                "FROM fact_sales f\n",
                "JOIN dim_region r ON f.region_key = r.region_key\n",
                "JOIN dim_channel c ON f.channel_key = c.channel_key\n",
                "GROUP BY r.region_name, c.trade_group_desc\n",
                "ORDER BY r.region_name, total_dollar_volume DESC\n",
                "\"\"\"\n",
                "spark.sql(query_1).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Sales by Brand per Month"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query_2 = \"\"\"\n",
                "SELECT \n",
                "    p.brand_nm,\n",
                "    d.year,\n",
                "    d.month,\n",
                "    SUM(f.dollar_volume) as total_volume\n",
                "FROM fact_sales f\n",
                "JOIN dim_product p ON f.product_key = p.product_key\n",
                "JOIN dim_date d ON f.date_key = d.date_key\n",
                "GROUP BY p.brand_nm, d.year, d.month\n",
                "ORDER BY p.brand_nm, d.year, d.month\n",
                "\"\"\"\n",
                "spark.sql(query_2).show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Observability & Control\n",
                "Analysis of pipeline execution metadata and data quality issues."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Process Control\n",
                "spark.read.format(\"delta\").load(f\"{DATA_DIR}/control/process_control\").createOrReplaceTempView(\"process_control\")\n",
                "\n",
                "# Load Quarantine (Handle if empty/missing)\n",
                "try:\n",
                "    spark.read.format(\"delta\").load(f\"{DATA_DIR}/control/quarantine\").createOrReplaceTempView(\"quarantine\")\n",
                "    print(\"Control & Quarantine tables loaded.\")\n",
                "except Exception as e:\n",
                "    print(f\"Quarantine table empty (Clean Run). Creating empty view for demo compatibility.\")\n",
                "    # Create empty DataFrame with correct schema\n",
                "    schema_ddl = \"quarantine_id STRING, batch_id STRING, source_table STRING, target_table STRING, record_data STRING, error_type STRING, error_code STRING, error_description STRING, dq_rule_name STRING, is_known_rule BOOLEAN, reprocessed BOOLEAN, reprocess_batch_id STRING, created_at TIMESTAMP, updated_at TIMESTAMP\"\n",
                "    spark.createDataFrame([], schema=schema_ddl).createOrReplaceTempView(\"quarantine\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Recent Pipeline Executions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query_control = \"\"\"\n",
                "SELECT \n",
                "    batch_id,\n",
                "    layer,\n",
                "    table_name,\n",
                "    status,\n",
                "    duration_seconds,\n",
                "    records_read,\n",
                "    records_written,\n",
                "    records_quarantined\n",
                "FROM process_control\n",
                "ORDER BY start_timestamp DESC\n",
                "LIMIT 20\n",
                "\"\"\"\n",
                "spark.sql(query_control).show(truncate=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Quarantine Analysis (if any)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.sql(\"SELECT * FROM quarantine LIMIT 10\").show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Visual Observability\n",
                "Graphical representation of execution metadata."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Fetch Process Control Data for Charts\n",
                "pdf_control = spark.sql(\"\"\"\n",
                "    SELECT table_name, records_read, records_written, records_quarantined \n",
                "    FROM process_control \n",
                "    WHERE batch_id = (SELECT MAX(batch_id) FROM process_control)\n",
                "\"\"\").toPandas()\n",
                "\n",
                "# Create Bar Chart\n",
                "if not pdf_control.empty:\n",
                "    plt.figure(figsize=(14, 7))\n",
                "    ax = pdf_control.set_index('table_name')[['records_read', 'records_written', 'records_quarantined']].plot(kind='bar', figsize=(14, 7))\n",
                "    plt.title('Records Processed per Table (Latest Batch)')\n",
                "    plt.ylabel('Count')\n",
                "    plt.xticks(rotation=45, ha='right')\n",
                "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
                "    plt.legend(title='Metrics')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No process data found to plot.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Governance & Architecture\n",
                "Evidence of the underlying engineering architecture for Data Governance and Observability.\n",
                "Although running locally, these configurations define the behavior in a production cloud environment."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Access Policies & Governance\n",
                "Configuration defining access control and data sensitivity levels (e.g. PII handling)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Determine project root to find config files\n",
                "# Assuming notebook is in 'notebooks/' directory\n",
                "PROJECT_ROOT = \"..\" if os.path.exists(\"../config\") else \".\"\n",
                "\n",
                "policy_path = f\"{PROJECT_ROOT}/config/governance_policies.yaml\"\n",
                "\n",
                "if os.path.exists(policy_path):\n",
                "    print(f\"--- Loading Policy Configuration: {policy_path} ---\\n\")\n",
                "    print(open(policy_path).read())\n",
                "else:\n",
                "    print(f\"Config file not found at {policy_path}. Checked at: {os.path.abspath(policy_path)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Observability & Alerting Rules\n",
                "Prometheus alert definitions for pipeline health monitoring."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "alert_path = f\"{PROJECT_ROOT}/config/alert_rules.yml\"\n",
                "\n",
                "if os.path.exists(alert_path):\n",
                "    print(f\"--- Loading Alert Rules: {alert_path} ---\\n\")\n",
                "    print(open(alert_path).read())\n",
                "else:\n",
                "    print(f\"Config file not found at {alert_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Data Catalog Integration (OpenMetadata)\n",
                "Source code responsible for registering table metadata and lineage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ingest_path = f\"{PROJECT_ROOT}/src/governance/ingest_metadata.py\"\n",
                "\n",
                "if os.path.exists(ingest_path):\n",
                "    print(f\"--- Source: {ingest_path} ---\\n\")\n",
                "    # Print first 50 lines to show import structure and class definition\n",
                "    with open(ingest_path) as f:\n",
                "        print(\"\".join(f.readlines()[:50]))\n",
                "        print(\"... (truncated)\")\n",
                "else:\n",
                "    print(f\"Source file not found at {ingest_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}